wandb: Currently logged in as: bismarckbamfo91. Use `wandb login --relogin` to force relogin
Requirement already satisfied: numpy in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.21.6)
Requirement already satisfied: pandas in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.3.5)
Requirement already satisfied: evaluate in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (0.4.0)
Requirement already satisfied: datasets in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (2.11.0)
Collecting argparse
  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)
Requirement already satisfied: scikit-learn in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (1.0.2)
Requirement already satisfied: transformers in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (4.27.4)
Requirement already satisfied: wandb in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (0.14.2)
Requirement already satisfied: matplotlib in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from -r requirements.txt (line 9)) (3.5.3)
Requirement already satisfied: seaborn in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from -r requirements.txt (line 10)) (0.12.2)
Requirement already satisfied: accelerate in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from -r requirements.txt (line 11)) (0.18.0)
Requirement already satisfied: python-dateutil>=2.7.3 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 2)) (2.8.2)
Requirement already satisfied: pytz>=2017.3 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 2)) (2023.3)
Requirement already satisfied: dill in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from evaluate->-r requirements.txt (line 3)) (0.3.6)
Requirement already satisfied: responses<0.19 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from evaluate->-r requirements.txt (line 3)) (0.18.0)
Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from evaluate->-r requirements.txt (line 3)) (2023.1.0)
Requirement already satisfied: huggingface-hub>=0.7.0 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from evaluate->-r requirements.txt (line 3)) (0.13.4)
Requirement already satisfied: packaging in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from evaluate->-r requirements.txt (line 3)) (23.0)
Requirement already satisfied: importlib-metadata in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from evaluate->-r requirements.txt (line 3)) (6.3.0)
Requirement already satisfied: xxhash in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from evaluate->-r requirements.txt (line 3)) (3.2.0)
Requirement already satisfied: tqdm>=4.62.1 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from evaluate->-r requirements.txt (line 3)) (4.65.0)
Requirement already satisfied: multiprocess in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from evaluate->-r requirements.txt (line 3)) (0.70.14)
Requirement already satisfied: requests>=2.19.0 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from evaluate->-r requirements.txt (line 3)) (2.28.2)
Requirement already satisfied: pyyaml>=5.1 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 4)) (6.0)
Requirement already satisfied: aiohttp in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 4)) (3.8.4)
Requirement already satisfied: pyarrow>=8.0.0 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 4)) (11.0.0)
Requirement already satisfied: threadpoolctl>=2.0.0 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from scikit-learn->-r requirements.txt (line 6)) (3.1.0)
Requirement already satisfied: scipy>=1.1.0 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from scikit-learn->-r requirements.txt (line 6)) (1.7.3)
Requirement already satisfied: joblib>=0.11 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from scikit-learn->-r requirements.txt (line 6)) (1.2.0)
Requirement already satisfied: filelock in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 7)) (3.11.0)
Requirement already satisfied: regex!=2019.12.17 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 7)) (2022.10.31)
Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 7)) (0.13.3)
Requirement already satisfied: docker-pycreds>=0.4.0 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 8)) (0.4.0)
Requirement already satisfied: sentry-sdk>=1.0.0 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 8)) (1.19.1)
Requirement already satisfied: pathtools in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 8)) (0.1.2)
Requirement already satisfied: appdirs>=1.4.3 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 8)) (1.4.4)
Requirement already satisfied: psutil>=5.0.0 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 8)) (5.9.4)
Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 8)) (4.22.1)
Requirement already satisfied: setuptools in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 8)) (65.6.3)
Requirement already satisfied: typing-extensions in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 8)) (4.5.0)
Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 8)) (3.1.31)
Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 8)) (8.1.3)
Requirement already satisfied: setproctitle in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 8)) (1.3.2)
Requirement already satisfied: cycler>=0.10 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 9)) (0.11.0)
Requirement already satisfied: kiwisolver>=1.0.1 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 9)) (1.4.4)
Requirement already satisfied: pyparsing>=2.2.1 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 9)) (3.0.9)
Requirement already satisfied: fonttools>=4.22.0 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 9)) (4.38.0)
Requirement already satisfied: pillow>=6.2.0 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 9)) (9.5.0)
Requirement already satisfied: torch>=1.4.0 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from accelerate->-r requirements.txt (line 11)) (1.13.1)
Requirement already satisfied: six>=1.4.0 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 8)) (1.16.0)
Requirement already satisfied: aiosignal>=1.1.2 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.3.1)
Requirement already satisfied: asynctest==0.13.0 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (0.13.0)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (4.0.2)
Requirement already satisfied: attrs>=17.3.0 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (22.2.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (6.0.4)
Requirement already satisfied: yarl<2.0,>=1.0 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.8.2)
Requirement already satisfied: frozenlist>=1.1.1 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.3.3)
Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (3.1.0)
Requirement already satisfied: gitdb<5,>=4.0.1 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 8)) (4.0.10)
Requirement already satisfied: certifi>=2017.4.17 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from requests>=2.19.0->evaluate->-r requirements.txt (line 3)) (2022.12.7)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from requests>=2.19.0->evaluate->-r requirements.txt (line 3)) (1.26.15)
Requirement already satisfied: idna<4,>=2.5 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from requests>=2.19.0->evaluate->-r requirements.txt (line 3)) (3.4)
Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from torch>=1.4.0->accelerate->-r requirements.txt (line 11)) (8.5.0.96)
Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from torch>=1.4.0->accelerate->-r requirements.txt (line 11)) (11.7.99)
Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from torch>=1.4.0->accelerate->-r requirements.txt (line 11)) (11.7.99)
Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from torch>=1.4.0->accelerate->-r requirements.txt (line 11)) (11.10.3.66)
Requirement already satisfied: wheel in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->accelerate->-r requirements.txt (line 11)) (0.38.4)
Requirement already satisfied: zipp>=0.5 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from importlib-metadata->evaluate->-r requirements.txt (line 3)) (3.15.0)
Requirement already satisfied: smmap<6,>=3.0.1 in /home/bodoom1/.conda/envs/myenv/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 8)) (5.0.0)
Installing collected packages: argparse
Successfully installed argparse-1.4.0
wandb: Currently logged in as: bismarckbamfo91. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /home/bodoom1/self_supervised_sp23/efficient-distillation/wandb/run-20230410_094506-mywix693
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 3e-06-42-rte-12
wandb: ⭐️ View project at https://wandb.ai/bismarckbamfo91/bert-large-glue-distillation
wandb: 🚀 View run at https://wandb.ai/bismarckbamfo91/bert-large-glue-distillation/runs/mywix693
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
load_checkpoint: False
epoch: 10
bs: 8
lr: 3e-06
task: rte
seed: 42
model_name: bert-large-uncased
max_len: 256
sage: False
plot_params: False
regularizer: None
weighted_dropout_iters: 0
use_sd: True
sd_alpha: 0.5
use_id: False
student_layer: 12
gradual: False
TRAIN Dataset: (2490, 2)
Epoch: 0, Iteration: 0, Loss:  0.7383478879928589
Epoch: 0, Iteration: 50, Loss:  0.6766648888587952
Epoch: 0, Iteration: 100, Loss:  0.7491518259048462
Epoch: 0, Iteration: 150, Loss:  0.661292552947998
Epoch: 0, Iteration: 200, Loss:  0.7270451188087463
Epoch: 0, Iteration: 250, Loss:  0.7083324193954468
Epoch: 0, Iteration: 300, Loss:  0.6588293313980103
Epoch: 0 used 102.89754271507263 seconds
epoch = 0 | acc = 0.51985559566787
Epoch: 1, Iteration: 0, Loss:  0.6812961101531982
Epoch: 1, Iteration: 50, Loss:  0.6914228796958923
Epoch: 1, Iteration: 100, Loss:  0.6941081881523132
Epoch: 1, Iteration: 150, Loss:  0.7079535126686096
Epoch: 1, Iteration: 200, Loss:  0.7066368460655212
Epoch: 1, Iteration: 250, Loss:  0.7402569651603699
Epoch: 1, Iteration: 300, Loss:  0.702858567237854
Epoch: 1 used 100.59223222732544 seconds
epoch = 1 | acc = 0.5451263537906137
Epoch: 2, Iteration: 0, Loss:  0.607391893863678
Epoch: 2, Iteration: 50, Loss:  0.6968638300895691
Epoch: 2, Iteration: 100, Loss:  0.6930768489837646
Epoch: 2, Iteration: 150, Loss:  0.6695102453231812
Epoch: 2, Iteration: 200, Loss:  0.6310206055641174
Epoch: 2, Iteration: 250, Loss:  0.6160867810249329
Epoch: 2, Iteration: 300, Loss:  0.6771731972694397
Epoch: 2 used 104.9602575302124 seconds
epoch = 2 | acc = 0.5631768953068592
Epoch: 3, Iteration: 0, Loss:  0.6069405674934387
Epoch: 3, Iteration: 50, Loss:  0.7149827480316162
Epoch: 3, Iteration: 100, Loss:  0.5892422199249268
Epoch: 3, Iteration: 150, Loss:  0.6393036842346191
Epoch: 3, Iteration: 200, Loss:  0.5959533452987671
Epoch: 3, Iteration: 250, Loss:  0.658237874507904
Epoch: 3, Iteration: 300, Loss:  0.631462812423706
Epoch: 3 used 101.01500511169434 seconds
epoch = 3 | acc = 0.5306859205776173
Epoch: 4, Iteration: 0, Loss:  0.6186631917953491
Epoch: 4, Iteration: 50, Loss:  0.5842454433441162
Epoch: 4, Iteration: 100, Loss:  0.5901004672050476
Epoch: 4, Iteration: 150, Loss:  0.6424302458763123
Epoch: 4, Iteration: 200, Loss:  0.6532893180847168
Epoch: 4, Iteration: 250, Loss:  0.5692815780639648
Epoch: 4, Iteration: 300, Loss:  0.6616272926330566
Epoch: 4 used 100.70511937141418 seconds
epoch = 4 | acc = 0.5306859205776173
Epoch: 5, Iteration: 0, Loss:  0.5966217517852783
Epoch: 5, Iteration: 50, Loss:  0.5524913668632507
Epoch: 5, Iteration: 100, Loss:  0.6002464294433594
Epoch: 5, Iteration: 150, Loss:  0.550555944442749
Epoch: 5, Iteration: 200, Loss:  0.5742945075035095
Epoch: 5, Iteration: 250, Loss:  0.6265748143196106
Epoch: 5, Iteration: 300, Loss:  0.6141374111175537
Epoch: 5 used 105.1713354587555 seconds
epoch = 5 | acc = 0.5379061371841155
Epoch: 6, Iteration: 0, Loss:  0.5411683320999146
Epoch: 6, Iteration: 50, Loss:  0.5566472411155701
Epoch: 6, Iteration: 100, Loss:  0.5643223524093628
Epoch: 6, Iteration: 150, Loss:  0.6507819890975952
Epoch: 6, Iteration: 200, Loss:  0.5599491000175476
Epoch: 6, Iteration: 250, Loss:  0.5009730458259583
Epoch: 6, Iteration: 300, Loss:  0.5635302066802979
Epoch: 6 used 101.09966206550598 seconds
epoch = 6 | acc = 0.5415162454873647
Epoch: 7, Iteration: 0, Loss:  0.5007795095443726
Epoch: 7, Iteration: 50, Loss:  0.5837538838386536
Epoch: 7, Iteration: 100, Loss:  0.505696177482605
Epoch: 7, Iteration: 150, Loss:  0.5102136135101318
Epoch: 7, Iteration: 200, Loss:  0.6098523139953613
Epoch: 7, Iteration: 250, Loss:  0.5353705286979675
Epoch: 7, Iteration: 300, Loss:  0.41767653822898865
Epoch: 7 used 100.08611226081848 seconds
epoch = 7 | acc = 0.5342960288808665
Epoch: 8, Iteration: 0, Loss:  0.571932315826416
Epoch: 8, Iteration: 50, Loss:  0.5980445742607117
Epoch: 8, Iteration: 100, Loss:  0.5567245483398438
Epoch: 8, Iteration: 150, Loss:  0.49905550479888916
Epoch: 8, Iteration: 200, Loss:  0.5812614560127258
Epoch: 8, Iteration: 250, Loss:  0.49421799182891846
Epoch: 8, Iteration: 300, Loss:  0.5808542370796204
Epoch: 8 used 105.24696636199951 seconds
epoch = 8 | acc = 0.5342960288808665
Epoch: 9, Iteration: 0, Loss:  0.5096009969711304
Epoch: 9, Iteration: 50, Loss:  0.45607689023017883
Epoch: 9, Iteration: 100, Loss:  0.410419762134552
Epoch: 9, Iteration: 150, Loss:  0.38033586740493774
Epoch: 9, Iteration: 200, Loss:  0.44002649188041687
Epoch: 9, Iteration: 250, Loss:  0.4266776740550995
Epoch: 9, Iteration: 300, Loss:  0.5463700294494629
Epoch: 9 used 100.78766083717346 seconds
epoch = 9 | acc = 0.5342960288808665
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: Training Loss █▇▆▇▇▇▇█▅▇▆▅▅▅▆▆▅▅▆▄▅▅▄▆▄▄▄▃▃▃▅▄▄▄▅▃▃▁▂▄
wandb: 
wandb: Run summary:
wandb: Training Loss 0.54637
wandb: 
wandb: 🚀 View run 3e-06-42-rte-12 at: https://wandb.ai/bismarckbamfo91/bert-large-glue-distillation/runs/mywix693
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230410_094506-mywix693/logs
wandb: Currently logged in as: bismarckbamfo91. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /home/bodoom1/self_supervised_sp23/efficient-distillation/wandb/run-20230410_100338-z6yq6ou0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 3e-06-42-rte-11
wandb: ⭐️ View project at https://wandb.ai/bismarckbamfo91/bert-large-glue-distillation
wandb: 🚀 View run at https://wandb.ai/bismarckbamfo91/bert-large-glue-distillation/runs/z6yq6ou0
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
load_checkpoint: False
epoch: 10
bs: 8
lr: 3e-06
task: rte
seed: 42
model_name: bert-large-uncased
max_len: 256
sage: False
plot_params: False
regularizer: None
weighted_dropout_iters: 0
use_sd: True
sd_alpha: 0.5
use_id: False
student_layer: 11
gradual: False
TRAIN Dataset: (2490, 2)
Epoch: 0, Iteration: 0, Loss:  0.7242894172668457
Epoch: 0, Iteration: 50, Loss:  0.6553013920783997
Epoch: 0, Iteration: 100, Loss:  0.7823511362075806
Epoch: 0, Iteration: 150, Loss:  0.6543399095535278
Epoch: 0, Iteration: 200, Loss:  0.7114326357841492
Epoch: 0, Iteration: 250, Loss:  0.6993440985679626
Epoch: 0, Iteration: 300, Loss:  0.6792670488357544
Epoch: 0 used 101.65918970108032 seconds
epoch = 0 | acc = 0.5415162454873647
Epoch: 1, Iteration: 0, Loss:  0.6650967001914978
Epoch: 1, Iteration: 50, Loss:  0.685674250125885
Epoch: 1, Iteration: 100, Loss:  0.6813524961471558
Epoch: 1, Iteration: 150, Loss:  0.6899627447128296
Epoch: 1, Iteration: 200, Loss:  0.6943628787994385
Epoch: 1, Iteration: 250, Loss:  0.7008078098297119
Epoch: 1, Iteration: 300, Loss:  0.6882425546646118
Epoch: 1 used 100.82859659194946 seconds
epoch = 1 | acc = 0.5234657039711191
Epoch: 2, Iteration: 0, Loss:  0.6277309656143188
Epoch: 2, Iteration: 50, Loss:  0.6939614415168762
Epoch: 2, Iteration: 100, Loss:  0.6657475233078003
Epoch: 2, Iteration: 150, Loss:  0.6751265525817871
Epoch: 2, Iteration: 200, Loss:  0.6258695721626282
Epoch: 2, Iteration: 250, Loss:  0.6434932351112366
Epoch: 2, Iteration: 300, Loss:  0.6599514484405518
Epoch: 2 used 105.52480101585388 seconds
epoch = 2 | acc = 0.5487364620938628
Epoch: 3, Iteration: 0, Loss:  0.6527166962623596
Epoch: 3, Iteration: 50, Loss:  0.6835677027702332
Epoch: 3, Iteration: 100, Loss:  0.5920376777648926
Epoch: 3, Iteration: 150, Loss:  0.680530846118927
Epoch: 3, Iteration: 200, Loss:  0.5958095192909241
Epoch: 3, Iteration: 250, Loss:  0.6771816611289978
Epoch: 3, Iteration: 300, Loss:  0.652419924736023
Epoch: 3 used 101.27225995063782 seconds
epoch = 3 | acc = 0.5631768953068592
Epoch: 4, Iteration: 0, Loss:  0.6143283247947693
Epoch: 4, Iteration: 50, Loss:  0.5900883078575134
Epoch: 4, Iteration: 100, Loss:  0.5944241881370544
Epoch: 4, Iteration: 150, Loss:  0.6254374980926514
Epoch: 4, Iteration: 200, Loss:  0.6933318972587585
Epoch: 4, Iteration: 250, Loss:  0.5361061096191406
Epoch: 4, Iteration: 300, Loss:  0.6289145350456238
Epoch: 4 used 100.67529773712158 seconds
epoch = 4 | acc = 0.5595667870036101
Epoch: 5, Iteration: 0, Loss:  0.5811700820922852
Epoch: 5, Iteration: 50, Loss:  0.5625734329223633
Epoch: 5, Iteration: 100, Loss:  0.5991706848144531
Epoch: 5, Iteration: 150, Loss:  0.546454906463623
Epoch: 5, Iteration: 200, Loss:  0.5833590626716614
Epoch: 5, Iteration: 250, Loss:  0.5814988017082214
Epoch: 5, Iteration: 300, Loss:  0.6528509855270386
Epoch: 5 used 105.29540944099426 seconds
epoch = 5 | acc = 0.5487364620938628
Epoch: 6, Iteration: 0, Loss:  0.5383079648017883
Epoch: 6, Iteration: 50, Loss:  0.5429909825325012
Epoch: 6, Iteration: 100, Loss:  0.5331488847732544
Epoch: 6, Iteration: 150, Loss:  0.5788387060165405
Epoch: 6, Iteration: 200, Loss:  0.654519259929657
Epoch: 6, Iteration: 250, Loss:  0.5364587306976318
Epoch: 6, Iteration: 300, Loss:  0.5601394772529602
Epoch: 6 used 101.2822265625 seconds
epoch = 6 | acc = 0.5631768953068592
Epoch: 7, Iteration: 0, Loss:  0.4896421432495117
Epoch: 7, Iteration: 50, Loss:  0.583697497844696
Epoch: 7, Iteration: 100, Loss:  0.49874475598335266
Epoch: 7, Iteration: 150, Loss:  0.5175175666809082
Epoch: 7, Iteration: 200, Loss:  0.6232245564460754
Epoch: 7, Iteration: 250, Loss:  0.5560727715492249
Epoch: 7, Iteration: 300, Loss:  0.4648451507091522
Epoch: 7 used 100.79209089279175 seconds
epoch = 7 | acc = 0.5487364620938628
Epoch: 8, Iteration: 0, Loss:  0.5569085478782654
Epoch: 8, Iteration: 50, Loss:  0.5445075631141663
Epoch: 8, Iteration: 100, Loss:  0.5491110682487488
Epoch: 8, Iteration: 150, Loss:  0.5274178981781006
Epoch: 8, Iteration: 200, Loss:  0.5881034731864929
Epoch: 8, Iteration: 250, Loss:  0.48932716250419617
Epoch: 8, Iteration: 300, Loss:  0.5524429082870483
Epoch: 8 used 105.2672758102417 seconds
epoch = 8 | acc = 0.5631768953068592
Epoch: 9, Iteration: 0, Loss:  0.5081462860107422
Epoch: 9, Iteration: 50, Loss:  0.5133827328681946
Epoch: 9, Iteration: 100, Loss:  0.45302656292915344
Epoch: 9, Iteration: 150, Loss:  0.42505472898483276
Epoch: 9, Iteration: 200, Loss:  0.4789348244667053
Epoch: 9, Iteration: 250, Loss:  0.5047743916511536
Epoch: 9, Iteration: 300, Loss:  0.4932340681552887
Epoch: 9 used 100.69839286804199 seconds
epoch = 9 | acc = 0.555956678700361
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: Training Loss █▆▆▇▆▇▇▇▆▇▇▆▆▅▇▇▅▅▅▃▄▅▃▄▃▃▆▃▂▂▅▄▄▃▄▂▂▁▂▂
wandb: 
wandb: Run summary:
wandb: Training Loss 0.49323
wandb: 
wandb: 🚀 View run 3e-06-42-rte-11 at: https://wandb.ai/bismarckbamfo91/bert-large-glue-distillation/runs/z6yq6ou0
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230410_100338-z6yq6ou0/logs
wandb: Currently logged in as: bismarckbamfo91. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /home/bodoom1/self_supervised_sp23/efficient-distillation/wandb/run-20230410_102205-9gklx8f5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 3e-06-42-rte-10
wandb: ⭐️ View project at https://wandb.ai/bismarckbamfo91/bert-large-glue-distillation
wandb: 🚀 View run at https://wandb.ai/bismarckbamfo91/bert-large-glue-distillation/runs/9gklx8f5
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
load_checkpoint: False
epoch: 10
bs: 8
lr: 3e-06
task: rte
seed: 42
model_name: bert-large-uncased
max_len: 256
sage: False
plot_params: False
regularizer: None
weighted_dropout_iters: 0
use_sd: True
sd_alpha: 0.5
use_id: False
student_layer: 10
gradual: False
TRAIN Dataset: (2490, 2)
Epoch: 0, Iteration: 0, Loss:  0.642564058303833
Epoch: 0, Iteration: 50, Loss:  0.6895187497138977
Epoch: 0, Iteration: 100, Loss:  0.7466195225715637
Epoch: 0, Iteration: 150, Loss:  0.6764388084411621
Epoch: 0, Iteration: 200, Loss:  0.69406658411026
Epoch: 0, Iteration: 250, Loss:  0.6851861476898193
Epoch: 0, Iteration: 300, Loss:  0.6594654321670532
Epoch: 0 used 101.56112504005432 seconds
epoch = 0 | acc = 0.5342960288808665
Epoch: 1, Iteration: 0, Loss:  0.6606713533401489
Epoch: 1, Iteration: 50, Loss:  0.7017602324485779
Epoch: 1, Iteration: 100, Loss:  0.6859308481216431
Epoch: 1, Iteration: 150, Loss:  0.7126186490058899
Epoch: 1, Iteration: 200, Loss:  0.6905036568641663
Epoch: 1, Iteration: 250, Loss:  0.7212061285972595
Epoch: 1, Iteration: 300, Loss:  0.6986180543899536
Epoch: 1 used 100.88853788375854 seconds
epoch = 1 | acc = 0.5523465703971119
Epoch: 2, Iteration: 0, Loss:  0.6521026492118835
Epoch: 2, Iteration: 50, Loss:  0.6703972816467285
Epoch: 2, Iteration: 100, Loss:  0.6864078640937805
Epoch: 2, Iteration: 150, Loss:  0.6564602851867676
Epoch: 2, Iteration: 200, Loss:  0.5871869921684265
Epoch: 2, Iteration: 250, Loss:  0.6461232900619507
Epoch: 2, Iteration: 300, Loss:  0.6929291486740112
Epoch: 2 used 105.28993105888367 seconds
epoch = 2 | acc = 0.5415162454873647
Epoch: 3, Iteration: 0, Loss:  0.6170905828475952
Epoch: 3, Iteration: 50, Loss:  0.6933737397193909
Epoch: 3, Iteration: 100, Loss:  0.6151543259620667
Epoch: 3, Iteration: 150, Loss:  0.6605814099311829
Epoch: 3, Iteration: 200, Loss:  0.6160990595817566
Epoch: 3, Iteration: 250, Loss:  0.6380337476730347
Epoch: 3, Iteration: 300, Loss:  0.625505268573761
Epoch: 3 used 101.28386402130127 seconds
epoch = 3 | acc = 0.516245487364621
Epoch: 4, Iteration: 0, Loss:  0.5882278680801392
Epoch: 4, Iteration: 50, Loss:  0.5855915546417236
Epoch: 4, Iteration: 100, Loss:  0.6148278713226318
Epoch: 4, Iteration: 150, Loss:  0.6286259889602661
Epoch: 4, Iteration: 200, Loss:  0.6638147234916687
Epoch: 4, Iteration: 250, Loss:  0.5316690802574158
Epoch: 4, Iteration: 300, Loss:  0.6301994919776917
Epoch: 4 used 100.4834930896759 seconds
epoch = 4 | acc = 0.5523465703971119
Epoch: 5, Iteration: 0, Loss:  0.5536074042320251
Epoch: 5, Iteration: 50, Loss:  0.5631317496299744
Epoch: 5, Iteration: 100, Loss:  0.549720048904419
Epoch: 5, Iteration: 150, Loss:  0.5611329674720764
Epoch: 5, Iteration: 200, Loss:  0.5832680463790894
Epoch: 5, Iteration: 250, Loss:  0.5947626829147339
Epoch: 5, Iteration: 300, Loss:  0.5885905027389526
Epoch: 5 used 105.49491143226624 seconds
epoch = 5 | acc = 0.5451263537906137
Epoch: 6, Iteration: 0, Loss:  0.5357422828674316
Epoch: 6, Iteration: 50, Loss:  0.5515645742416382
Epoch: 6, Iteration: 100, Loss:  0.5682192444801331
Epoch: 6, Iteration: 150, Loss:  0.5990129113197327
Epoch: 6, Iteration: 200, Loss:  0.566975474357605
Epoch: 6, Iteration: 250, Loss:  0.5106385946273804
Epoch: 6, Iteration: 300, Loss:  0.5718438029289246
Epoch: 6 used 101.34860754013062 seconds
epoch = 6 | acc = 0.5342960288808665
Epoch: 7, Iteration: 0, Loss:  0.5200271010398865
Epoch: 7, Iteration: 50, Loss:  0.5656014680862427
Epoch: 7, Iteration: 100, Loss:  0.48457589745521545
Epoch: 7, Iteration: 150, Loss:  0.4681399464607239
Epoch: 7, Iteration: 200, Loss:  0.6284895539283752
Epoch: 7, Iteration: 250, Loss:  0.5284147262573242
Epoch: 7, Iteration: 300, Loss:  0.4308105707168579
Epoch: 7 used 100.12001419067383 seconds
epoch = 7 | acc = 0.5342960288808665
Epoch: 8, Iteration: 0, Loss:  0.5525814890861511
Epoch: 8, Iteration: 50, Loss:  0.5497774481773376
Epoch: 8, Iteration: 100, Loss:  0.5576000809669495
Epoch: 8, Iteration: 150, Loss:  0.49074065685272217
Epoch: 8, Iteration: 200, Loss:  0.6017264127731323
Epoch: 8, Iteration: 250, Loss:  0.47027474641799927
Epoch: 8, Iteration: 300, Loss:  0.5371137857437134
Epoch: 8 used 105.07980370521545 seconds
epoch = 8 | acc = 0.5415162454873647
Epoch: 9, Iteration: 0, Loss:  0.46301132440567017
Epoch: 9, Iteration: 50, Loss:  0.4775889813899994
Epoch: 9, Iteration: 100, Loss:  0.36159974336624146
Epoch: 9, Iteration: 150, Loss:  0.3664131760597229
Epoch: 9, Iteration: 200, Loss:  0.4777384102344513
Epoch: 9, Iteration: 250, Loss:  0.47175613045692444
Epoch: 9, Iteration: 300, Loss:  0.48045283555984497
Epoch: 9 used 101.43096280097961 seconds
epoch = 9 | acc = 0.5451263537906137
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: Training Loss ▆▇▇▇▇███▇▇▇▇▆▆▇▆▅▆▆▄▅▅▅▆▄▅▅▄▄▃▆▄▅▅▆▃▃▁▃▃
wandb: 
wandb: Run summary:
wandb: Training Loss 0.48045
wandb: 
wandb: 🚀 View run 3e-06-42-rte-10 at: https://wandb.ai/bismarckbamfo91/bert-large-glue-distillation/runs/9gklx8f5
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230410_102205-9gklx8f5/logs
wandb: Currently logged in as: bismarckbamfo91. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /home/bodoom1/self_supervised_sp23/efficient-distillation/wandb/run-20230410_104030-tbziy57k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 3e-06-42-rte-9
wandb: ⭐️ View project at https://wandb.ai/bismarckbamfo91/bert-large-glue-distillation
wandb: 🚀 View run at https://wandb.ai/bismarckbamfo91/bert-large-glue-distillation/runs/tbziy57k
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
load_checkpoint: False
epoch: 10
bs: 8
lr: 3e-06
task: rte
seed: 42
model_name: bert-large-uncased
max_len: 256
sage: False
plot_params: False
regularizer: None
weighted_dropout_iters: 0
use_sd: True
sd_alpha: 0.5
use_id: False
student_layer: 9
gradual: False
TRAIN Dataset: (2490, 2)
Epoch: 0, Iteration: 0, Loss:  0.7748104929924011
Epoch: 0, Iteration: 50, Loss:  0.6664252877235413
Epoch: 0, Iteration: 100, Loss:  0.7561363577842712
Epoch: 0, Iteration: 150, Loss:  0.67121422290802
Epoch: 0, Iteration: 200, Loss:  0.6993505358695984
Epoch: 0, Iteration: 250, Loss:  0.6677812337875366
Epoch: 0, Iteration: 300, Loss:  0.6968043446540833
Epoch: 0 used 101.80419182777405 seconds
epoch = 0 | acc = 0.5306859205776173
Epoch: 1, Iteration: 0, Loss:  0.6942543387413025
Epoch: 1, Iteration: 50, Loss:  0.6932770609855652
Epoch: 1, Iteration: 100, Loss:  0.6667197346687317
Epoch: 1, Iteration: 150, Loss:  0.7065540552139282
Epoch: 1, Iteration: 200, Loss:  0.7104718089103699
Epoch: 1, Iteration: 250, Loss:  0.7179744243621826
Epoch: 1, Iteration: 300, Loss:  0.6908015012741089
Epoch: 1 used 100.93108892440796 seconds
epoch = 1 | acc = 0.5270758122743683
Epoch: 2, Iteration: 0, Loss:  0.6325182318687439
Epoch: 2, Iteration: 50, Loss:  0.6724133491516113
Epoch: 2, Iteration: 100, Loss:  0.6773310899734497
Epoch: 2, Iteration: 150, Loss:  0.666126549243927
Epoch: 2, Iteration: 200, Loss:  0.6249805688858032
Epoch: 2, Iteration: 250, Loss:  0.6685716509819031
Epoch: 2, Iteration: 300, Loss:  0.6618351340293884
Epoch: 2 used 105.26876640319824 seconds
epoch = 2 | acc = 0.5415162454873647
Epoch: 3, Iteration: 0, Loss:  0.6723750829696655
Epoch: 3, Iteration: 50, Loss:  0.7375273704528809
Epoch: 3, Iteration: 100, Loss:  0.6308839321136475
Epoch: 3, Iteration: 150, Loss:  0.6941738128662109
Epoch: 3, Iteration: 200, Loss:  0.6317985653877258
Epoch: 3, Iteration: 250, Loss:  0.6720943450927734
Epoch: 3, Iteration: 300, Loss:  0.6431183815002441
Epoch: 3 used 100.90179967880249 seconds
epoch = 3 | acc = 0.5342960288808665
Epoch: 4, Iteration: 0, Loss:  0.6066244840621948
Epoch: 4, Iteration: 50, Loss:  0.6051867008209229
Epoch: 4, Iteration: 100, Loss:  0.6162494421005249
Epoch: 4, Iteration: 150, Loss:  0.6233422160148621
Epoch: 4, Iteration: 200, Loss:  0.6866238117218018
Epoch: 4, Iteration: 250, Loss:  0.566185474395752
Epoch: 4, Iteration: 300, Loss:  0.6526243686676025
Epoch: 4 used 100.94776344299316 seconds
epoch = 4 | acc = 0.5667870036101083
Epoch: 5, Iteration: 0, Loss:  0.603107750415802
Epoch: 5, Iteration: 50, Loss:  0.5683160424232483
Epoch: 5, Iteration: 100, Loss:  0.5905987620353699
Epoch: 5, Iteration: 150, Loss:  0.6220604181289673
Epoch: 5, Iteration: 200, Loss:  0.6206110119819641
Epoch: 5, Iteration: 250, Loss:  0.5836911797523499
Epoch: 5, Iteration: 300, Loss:  0.6262785196304321
Epoch: 5 used 105.42536878585815 seconds
epoch = 5 | acc = 0.5776173285198556
Epoch: 6, Iteration: 0, Loss:  0.5835067629814148
Epoch: 6, Iteration: 50, Loss:  0.5625888705253601
Epoch: 6, Iteration: 100, Loss:  0.5559657216072083
Epoch: 6, Iteration: 150, Loss:  0.6114161610603333
Epoch: 6, Iteration: 200, Loss:  0.5772207379341125
Epoch: 6, Iteration: 250, Loss:  0.5757907629013062
Epoch: 6, Iteration: 300, Loss:  0.6004080772399902
Epoch: 6 used 101.35335803031921 seconds
epoch = 6 | acc = 0.555956678700361
Epoch: 7, Iteration: 0, Loss:  0.5337156057357788
Epoch: 7, Iteration: 50, Loss:  0.5624688863754272
Epoch: 7, Iteration: 100, Loss:  0.5338568091392517
Epoch: 7, Iteration: 150, Loss:  0.5119669437408447
Epoch: 7, Iteration: 200, Loss:  0.6322382092475891
Epoch: 7, Iteration: 250, Loss:  0.5548913478851318
Epoch: 7, Iteration: 300, Loss:  0.5075591802597046
Epoch: 7 used 100.44467186927795 seconds
epoch = 7 | acc = 0.5740072202166066
Epoch: 8, Iteration: 0, Loss:  0.5465805530548096
Epoch: 8, Iteration: 50, Loss:  0.5993427634239197
Epoch: 8, Iteration: 100, Loss:  0.5373317003250122
Epoch: 8, Iteration: 150, Loss:  0.612766683101654
Epoch: 8, Iteration: 200, Loss:  0.616195797920227
Epoch: 8, Iteration: 250, Loss:  0.5599895119667053
Epoch: 8, Iteration: 300, Loss:  0.5494997501373291
Epoch: 8 used 105.34839081764221 seconds
epoch = 8 | acc = 0.5487364620938628
Epoch: 9, Iteration: 0, Loss:  0.5330997705459595
Epoch: 9, Iteration: 50, Loss:  0.5618292093276978
Epoch: 9, Iteration: 100, Loss:  0.5520049333572388
Epoch: 9, Iteration: 150, Loss:  0.4867636263370514
Epoch: 9, Iteration: 200, Loss:  0.5615636706352234
Epoch: 9, Iteration: 250, Loss:  0.5327569246292114
Epoch: 9, Iteration: 300, Loss:  0.5405662655830383
Epoch: 9 used 101.18721723556519 seconds
epoch = 9 | acc = 0.5595667870036101
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: Training Loss █▅▅▅▆▆▆▆▄▅▅▅▅▄▆▅▃▃▄▂▃▃▄▂▂▂▂▂▁▁▄▂▁▁▃▂▁▂▂▁
wandb: 
wandb: Run summary:
wandb: Training Loss 0.54057
wandb: 
wandb: 🚀 View run 3e-06-42-rte-9 at: https://wandb.ai/bismarckbamfo91/bert-large-glue-distillation/runs/tbziy57k
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230410_104030-tbziy57k/logs
wandb: Currently logged in as: bismarckbamfo91. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /home/bodoom1/self_supervised_sp23/efficient-distillation/wandb/run-20230410_105857-nteseklp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 3e-06-42-rte-8
wandb: ⭐️ View project at https://wandb.ai/bismarckbamfo91/bert-large-glue-distillation
wandb: 🚀 View run at https://wandb.ai/bismarckbamfo91/bert-large-glue-distillation/runs/nteseklp
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
load_checkpoint: False
epoch: 10
bs: 8
lr: 3e-06
task: rte
seed: 42
model_name: bert-large-uncased
max_len: 256
sage: False
plot_params: False
regularizer: None
weighted_dropout_iters: 0
use_sd: True
sd_alpha: 0.5
use_id: False
student_layer: 8
gradual: False
TRAIN Dataset: (2490, 2)
Epoch: 0, Iteration: 0, Loss:  0.747222363948822
Epoch: 0, Iteration: 50, Loss:  0.6765332818031311
Epoch: 0, Iteration: 100, Loss:  0.7154256105422974
Epoch: 0, Iteration: 150, Loss:  0.6321051716804504
Epoch: 0, Iteration: 200, Loss:  0.6979345083236694
Epoch: 0, Iteration: 250, Loss:  0.7064439058303833
Epoch: 0, Iteration: 300, Loss:  0.664813756942749
Epoch: 0 used 101.83982586860657 seconds
epoch = 0 | acc = 0.5234657039711191
Epoch: 1, Iteration: 0, Loss:  0.6774407625198364
Epoch: 1, Iteration: 50, Loss:  0.6947965621948242
Epoch: 1, Iteration: 100, Loss:  0.6760629415512085
Epoch: 1, Iteration: 150, Loss:  0.7264466881752014
Epoch: 1, Iteration: 200, Loss:  0.6871369481086731
Epoch: 1, Iteration: 250, Loss:  0.7075839042663574
Epoch: 1, Iteration: 300, Loss:  0.6766977310180664
Epoch: 1 used 100.7920491695404 seconds
epoch = 1 | acc = 0.555956678700361
Epoch: 2, Iteration: 0, Loss:  0.6305025219917297
Epoch: 2, Iteration: 50, Loss:  0.6728472113609314
Epoch: 2, Iteration: 100, Loss:  0.6538455486297607
Epoch: 2, Iteration: 150, Loss:  0.6606487035751343
Epoch: 2, Iteration: 200, Loss:  0.6092398166656494
Epoch: 2, Iteration: 250, Loss:  0.6213172674179077
Epoch: 2, Iteration: 300, Loss:  0.6322042346000671
Epoch: 2 used 105.60489130020142 seconds
epoch = 2 | acc = 0.5703971119133574
Epoch: 3, Iteration: 0, Loss:  0.6353456974029541
Epoch: 3, Iteration: 50, Loss:  0.7128865718841553
Epoch: 3, Iteration: 100, Loss:  0.574005126953125
Epoch: 3, Iteration: 150, Loss:  0.6528624296188354
Epoch: 3, Iteration: 200, Loss:  0.6151809692382812
Epoch: 3, Iteration: 250, Loss:  0.6712421178817749
Epoch: 3, Iteration: 300, Loss:  0.6278887987136841
Epoch: 3 used 101.41429567337036 seconds
epoch = 3 | acc = 0.5342960288808665
Epoch: 4, Iteration: 0, Loss:  0.5899001359939575
Epoch: 4, Iteration: 50, Loss:  0.5826682448387146
Epoch: 4, Iteration: 100, Loss:  0.5981003046035767
Epoch: 4, Iteration: 150, Loss:  0.6630730628967285
Epoch: 4, Iteration: 200, Loss:  0.6572412848472595
Epoch: 4, Iteration: 250, Loss:  0.5500006675720215
Epoch: 4, Iteration: 300, Loss:  0.6716082096099854
Epoch: 4 used 100.75854063034058 seconds
epoch = 4 | acc = 0.5812274368231047
Epoch: 5, Iteration: 0, Loss:  0.5671620965003967
Epoch: 5, Iteration: 50, Loss:  0.5789598822593689
Epoch: 5, Iteration: 100, Loss:  0.5677751302719116
Epoch: 5, Iteration: 150, Loss:  0.5574639439582825
Epoch: 5, Iteration: 200, Loss:  0.5846633911132812
Epoch: 5, Iteration: 250, Loss:  0.6287356019020081
Epoch: 5, Iteration: 300, Loss:  0.5931593775749207
Epoch: 5 used 105.39962124824524 seconds
epoch = 5 | acc = 0.5884476534296029
Epoch: 6, Iteration: 0, Loss:  0.5868847370147705
Epoch: 6, Iteration: 50, Loss:  0.5481998324394226
Epoch: 6, Iteration: 100, Loss:  0.5490667223930359
Epoch: 6, Iteration: 150, Loss:  0.5554682016372681
Epoch: 6, Iteration: 200, Loss:  0.5925467014312744
Epoch: 6, Iteration: 250, Loss:  0.5466079115867615
Epoch: 6, Iteration: 300, Loss:  0.5462808012962341
Epoch: 6 used 100.97966623306274 seconds
epoch = 6 | acc = 0.5631768953068592
Epoch: 7, Iteration: 0, Loss:  0.5122172832489014
Epoch: 7, Iteration: 50, Loss:  0.5516798496246338
Epoch: 7, Iteration: 100, Loss:  0.5432702302932739
Epoch: 7, Iteration: 150, Loss:  0.5101276636123657
Epoch: 7, Iteration: 200, Loss:  0.6229878664016724
Epoch: 7, Iteration: 250, Loss:  0.5334251523017883
Epoch: 7, Iteration: 300, Loss:  0.4937032461166382
Epoch: 7 used 100.80974054336548 seconds
epoch = 7 | acc = 0.5776173285198556
Epoch: 8, Iteration: 0, Loss:  0.5633689761161804
Epoch: 8, Iteration: 50, Loss:  0.5746577382087708
Epoch: 8, Iteration: 100, Loss:  0.5292855501174927
Epoch: 8, Iteration: 150, Loss:  0.5341492891311646
Epoch: 8, Iteration: 200, Loss:  0.6000967025756836
Epoch: 8, Iteration: 250, Loss:  0.5120377540588379
Epoch: 8, Iteration: 300, Loss:  0.5515661239624023
Epoch: 8 used 105.41678380966187 seconds
epoch = 8 | acc = 0.5776173285198556
Epoch: 9, Iteration: 0, Loss:  0.5486904978752136
Epoch: 9, Iteration: 50, Loss:  0.5599991083145142
Epoch: 9, Iteration: 100, Loss:  0.5292046070098877
Epoch: 9, Iteration: 150, Loss:  0.44215720891952515
Epoch: 9, Iteration: 200, Loss:  0.5761592984199524
Epoch: 9, Iteration: 250, Loss:  0.5668478012084961
Epoch: 9, Iteration: 300, Loss:  0.5214384198188782
Epoch: 9 used 101.09543013572693 seconds
epoch = 9 | acc = 0.5595667870036101
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: Training Loss █▆▅▇▆▆▇▇▅▆▅▄▅▃▅▆▃▄▅▂▃▃▂▄▃▂▃▂▁▂▄▂▃▂▄▁▂▂▃▁
wandb: 
wandb: Run summary:
wandb: Training Loss 0.52144
wandb: 
wandb: 🚀 View run 3e-06-42-rte-8 at: https://wandb.ai/bismarckbamfo91/bert-large-glue-distillation/runs/nteseklp
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230410_105857-nteseklp/logs

Found cached dataset boolq (/home/tli104/.cache/huggingface/datasets/boolq/default/0.1.0/bf0dd57da941c50de94ae3ce3cef7fea48c08f337a4b7aac484e9dddc5aa24e5)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 708.92it/s]
Loading cached shuffled indices for dataset at /home/tli104/.cache/huggingface/datasets/boolq/default/0.1.0/bf0dd57da941c50de94ae3ce3cef7fea48c08f337a4b7aac484e9dddc5aa24e5/cache-12096f4bc449d078.arrow
Traceback (most recent call last):
  File "bloom.py", line 78, in <module>
    generate_text(args)
  File "bloom.py", line 20, in generate_text
    output = model.generate(input_ids, max_new_tokens=args.max_new_tokens, do_sample=args.do_sample, \
  File "/home/tli104/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/tli104/.local/lib/python3.8/site-packages/transformers/generation/utils.py", line 1437, in generate
    return self.sample(
  File "/home/tli104/.local/lib/python3.8/site-packages/transformers/generation/utils.py", line 2443, in sample
    outputs = self(
  File "/home/tli104/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/tli104/.local/lib/python3.8/site-packages/transformers/models/bloom/modeling_bloom.py", line 900, in forward
    transformer_outputs = self.transformer(
  File "/home/tli104/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/tli104/.local/lib/python3.8/site-packages/transformers/models/bloom/modeling_bloom.py", line 782, in forward
    outputs = block(
  File "/home/tli104/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/tli104/.local/lib/python3.8/site-packages/transformers/models/bloom/modeling_bloom.py", line 439, in forward
    attn_outputs = self.self_attention(
  File "/home/tli104/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/tli104/.local/lib/python3.8/site-packages/transformers/models/bloom/modeling_bloom.py", line 333, in forward
    attention_probs = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(input_dtype)
  File "/home/tli104/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1843, in softmax
    ret = input.softmax(dim, dtype=dtype)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.62 GiB (GPU 0; 39.59 GiB total capacity; 21.75 GiB already allocated; 7.44 GiB free; 30.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: gpu15: task 0: Exited with exit code 1

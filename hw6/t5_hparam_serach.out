Could not find conda environment: toy_classification_env
You can list all discoverable environments with `conda info --envs`.

Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: torch in /home/tli104/.local/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.13.1)
Requirement already satisfied: scikit-learn==0.24.2 in /home/tli104/.local/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (0.24.2)
Requirement already satisfied: tqdm==4.64.1 in /home/tli104/.local/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (4.64.1)
Requirement already satisfied: transformers==4.26.1 in /home/tli104/.local/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (4.26.1)
Requirement already satisfied: datasets==2.10.0 in /home/tli104/.local/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (2.10.0)
Requirement already satisfied: evaluate==0.4.0 in /home/tli104/.local/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (0.4.0)
Requirement already satisfied: typing-extensions in /home/tli104/.local/lib/python3.8/site-packages (from torch->-r requirements.txt (line 1)) (4.5.0)
Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66; platform_system == "Linux" in /home/tli104/.local/lib/python3.8/site-packages (from torch->-r requirements.txt (line 1)) (11.10.3.66)
Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99; platform_system == "Linux" in /home/tli104/.local/lib/python3.8/site-packages (from torch->-r requirements.txt (line 1)) (11.7.99)
Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96; platform_system == "Linux" in /home/tli104/.local/lib/python3.8/site-packages (from torch->-r requirements.txt (line 1)) (8.5.0.96)
Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == "Linux" in /home/tli104/.local/lib/python3.8/site-packages (from torch->-r requirements.txt (line 1)) (11.7.99)
Requirement already satisfied: numpy>=1.13.3 in /data/apps/linux-centos8-cascadelake/gcc-9.3.0/anaconda3-2020.07-i7qavhiohb2uwqs4eqjeefzx3kp5jqdu/lib/python3.8/site-packages (from scikit-learn==0.24.2->-r requirements.txt (line 2)) (1.18.5)
Requirement already satisfied: scipy>=0.19.1 in /data/apps/linux-centos8-cascadelake/gcc-9.3.0/anaconda3-2020.07-i7qavhiohb2uwqs4eqjeefzx3kp5jqdu/lib/python3.8/site-packages (from scikit-learn==0.24.2->-r requirements.txt (line 2)) (1.5.0)
Requirement already satisfied: threadpoolctl>=2.0.0 in /data/apps/linux-centos8-cascadelake/gcc-9.3.0/anaconda3-2020.07-i7qavhiohb2uwqs4eqjeefzx3kp5jqdu/lib/python3.8/site-packages (from scikit-learn==0.24.2->-r requirements.txt (line 2)) (2.1.0)
Requirement already satisfied: joblib>=0.11 in /data/apps/linux-centos8-cascadelake/gcc-9.3.0/anaconda3-2020.07-i7qavhiohb2uwqs4eqjeefzx3kp5jqdu/lib/python3.8/site-packages (from scikit-learn==0.24.2->-r requirements.txt (line 2)) (0.16.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/tli104/.local/lib/python3.8/site-packages (from transformers==4.26.1->-r requirements.txt (line 4)) (0.13.1)
Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/tli104/.local/lib/python3.8/site-packages (from transformers==4.26.1->-r requirements.txt (line 4)) (0.13.2)
Requirement already satisfied: packaging>=20.0 in /data/apps/linux-centos8-cascadelake/gcc-9.3.0/anaconda3-2020.07-i7qavhiohb2uwqs4eqjeefzx3kp5jqdu/lib/python3.8/site-packages (from transformers==4.26.1->-r requirements.txt (line 4)) (20.4)
Requirement already satisfied: pyyaml>=5.1 in /data/apps/linux-centos8-cascadelake/gcc-9.3.0/anaconda3-2020.07-i7qavhiohb2uwqs4eqjeefzx3kp5jqdu/lib/python3.8/site-packages (from transformers==4.26.1->-r requirements.txt (line 4)) (5.3.1)
Requirement already satisfied: requests in /data/apps/linux-centos8-cascadelake/gcc-9.3.0/anaconda3-2020.07-i7qavhiohb2uwqs4eqjeefzx3kp5jqdu/lib/python3.8/site-packages (from transformers==4.26.1->-r requirements.txt (line 4)) (2.24.0)
Requirement already satisfied: regex!=2019.12.17 in /data/apps/linux-centos8-cascadelake/gcc-9.3.0/anaconda3-2020.07-i7qavhiohb2uwqs4eqjeefzx3kp5jqdu/lib/python3.8/site-packages (from transformers==4.26.1->-r requirements.txt (line 4)) (2020.6.8)
Requirement already satisfied: filelock in /data/apps/linux-centos8-cascadelake/gcc-9.3.0/anaconda3-2020.07-i7qavhiohb2uwqs4eqjeefzx3kp5jqdu/lib/python3.8/site-packages (from transformers==4.26.1->-r requirements.txt (line 4)) (3.0.12)
Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/tli104/.local/lib/python3.8/site-packages (from datasets==2.10.0->-r requirements.txt (line 5)) (0.3.6)
Requirement already satisfied: aiohttp in /home/tli104/.local/lib/python3.8/site-packages (from datasets==2.10.0->-r requirements.txt (line 5)) (3.8.4)
Requirement already satisfied: pandas in /data/apps/linux-centos8-cascadelake/gcc-9.3.0/anaconda3-2020.07-i7qavhiohb2uwqs4eqjeefzx3kp5jqdu/lib/python3.8/site-packages (from datasets==2.10.0->-r requirements.txt (line 5)) (1.0.5)
Requirement already satisfied: multiprocess in /home/tli104/.local/lib/python3.8/site-packages (from datasets==2.10.0->-r requirements.txt (line 5)) (0.70.14)
Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/tli104/.local/lib/python3.8/site-packages (from datasets==2.10.0->-r requirements.txt (line 5)) (2023.3.0)
Requirement already satisfied: responses<0.19 in /home/tli104/.local/lib/python3.8/site-packages (from datasets==2.10.0->-r requirements.txt (line 5)) (0.18.0)
Requirement already satisfied: xxhash in /home/tli104/.local/lib/python3.8/site-packages (from datasets==2.10.0->-r requirements.txt (line 5)) (3.2.0)
Requirement already satisfied: pyarrow>=6.0.0 in /home/tli104/.local/lib/python3.8/site-packages (from datasets==2.10.0->-r requirements.txt (line 5)) (11.0.0)
Requirement already satisfied: wheel in /data/apps/linux-centos8-cascadelake/gcc-9.3.0/anaconda3-2020.07-i7qavhiohb2uwqs4eqjeefzx3kp5jqdu/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66; platform_system == "Linux"->torch->-r requirements.txt (line 1)) (0.34.2)
Requirement already satisfied: setuptools in /data/apps/linux-centos8-cascadelake/gcc-9.3.0/anaconda3-2020.07-i7qavhiohb2uwqs4eqjeefzx3kp5jqdu/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66; platform_system == "Linux"->torch->-r requirements.txt (line 1)) (49.2.0.post20200714)
Requirement already satisfied: pyparsing>=2.0.2 in /data/apps/linux-centos8-cascadelake/gcc-9.3.0/anaconda3-2020.07-i7qavhiohb2uwqs4eqjeefzx3kp5jqdu/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.26.1->-r requirements.txt (line 4)) (2.4.7)
Requirement already satisfied: six in /data/apps/linux-centos8-cascadelake/gcc-9.3.0/anaconda3-2020.07-i7qavhiohb2uwqs4eqjeefzx3kp5jqdu/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.26.1->-r requirements.txt (line 4)) (1.15.0)
Requirement already satisfied: chardet<4,>=3.0.2 in /data/apps/linux-centos8-cascadelake/gcc-9.3.0/anaconda3-2020.07-i7qavhiohb2uwqs4eqjeefzx3kp5jqdu/lib/python3.8/site-packages (from requests->transformers==4.26.1->-r requirements.txt (line 4)) (3.0.4)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /data/apps/linux-centos8-cascadelake/gcc-9.3.0/anaconda3-2020.07-i7qavhiohb2uwqs4eqjeefzx3kp5jqdu/lib/python3.8/site-packages (from requests->transformers==4.26.1->-r requirements.txt (line 4)) (1.25.9)
Requirement already satisfied: idna<3,>=2.5 in /data/apps/linux-centos8-cascadelake/gcc-9.3.0/anaconda3-2020.07-i7qavhiohb2uwqs4eqjeefzx3kp5jqdu/lib/python3.8/site-packages (from requests->transformers==4.26.1->-r requirements.txt (line 4)) (2.10)
Requirement already satisfied: certifi>=2017.4.17 in /data/apps/linux-centos8-cascadelake/gcc-9.3.0/anaconda3-2020.07-i7qavhiohb2uwqs4eqjeefzx3kp5jqdu/lib/python3.8/site-packages (from requests->transformers==4.26.1->-r requirements.txt (line 4)) (2020.6.20)
Requirement already satisfied: aiosignal>=1.1.2 in /home/tli104/.local/lib/python3.8/site-packages (from aiohttp->datasets==2.10.0->-r requirements.txt (line 5)) (1.3.1)
Requirement already satisfied: multidict<7.0,>=4.5 in /home/tli104/.local/lib/python3.8/site-packages (from aiohttp->datasets==2.10.0->-r requirements.txt (line 5)) (6.0.4)
Requirement already satisfied: attrs>=17.3.0 in /data/apps/linux-centos8-cascadelake/gcc-9.3.0/anaconda3-2020.07-i7qavhiohb2uwqs4eqjeefzx3kp5jqdu/lib/python3.8/site-packages (from aiohttp->datasets==2.10.0->-r requirements.txt (line 5)) (19.3.0)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/tli104/.local/lib/python3.8/site-packages (from aiohttp->datasets==2.10.0->-r requirements.txt (line 5)) (4.0.2)
Requirement already satisfied: frozenlist>=1.1.1 in /home/tli104/.local/lib/python3.8/site-packages (from aiohttp->datasets==2.10.0->-r requirements.txt (line 5)) (1.3.3)
Requirement already satisfied: yarl<2.0,>=1.0 in /home/tli104/.local/lib/python3.8/site-packages (from aiohttp->datasets==2.10.0->-r requirements.txt (line 5)) (1.8.2)
Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/tli104/.local/lib/python3.8/site-packages (from aiohttp->datasets==2.10.0->-r requirements.txt (line 5)) (3.1.0)
Requirement already satisfied: python-dateutil>=2.6.1 in /data/apps/linux-centos8-cascadelake/gcc-9.3.0/anaconda3-2020.07-i7qavhiohb2uwqs4eqjeefzx3kp5jqdu/lib/python3.8/site-packages (from pandas->datasets==2.10.0->-r requirements.txt (line 5)) (2.8.1)
Requirement already satisfied: pytz>=2017.2 in /data/apps/linux-centos8-cascadelake/gcc-9.3.0/anaconda3-2020.07-i7qavhiohb2uwqs4eqjeefzx3kp5jqdu/lib/python3.8/site-packages (from pandas->datasets==2.10.0->-r requirements.txt (line 5)) (2020.1)
Found cached dataset boolq (/home/tli104/.cache/huggingface/datasets/boolq/default/0.1.0/bf0dd57da941c50de94ae3ce3cef7fea48c08f337a4b7aac484e9dddc5aa24e5)
Specified arguments: Namespace(batch_size=32, device='cuda', experiment='full', lr=0.0001, model='t5-small', num_epochs=9, small_subset=False)
Loading the dataset ...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 867.67it/s]
/home/tli104/.local/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.
For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.
- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.
- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.
- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.
  warnings.warn(
Slicing the data...
Size of the loaded dataset:
 - train: 8000
 - dev: 3270
 - test: 1427
Loading the tokenizer...
Loding the data into DS...
 >>>>>>>> Initializing the data loaders ... 
Loading the model ...
Moving model to device ...cuda
 >>>>>>>>  Starting training ... 
 >>>>>>>>  Initializing optimizer
Epoch 1 training:
 ===> Epoch 1
 - Average training metrics: accuracy={'accuracy': 0.57675}
 - Average validation metrics: accuracy={'accuracy': 0.6259938837920489}
 - Average test metrics: accuracy={'accuracy': 0.6384022424667134}
Epoch 2 training:
 ===> Epoch 2
 - Average training metrics: accuracy={'accuracy': 0.6645}
 - Average validation metrics: accuracy={'accuracy': 0.6801223241590214}
 - Average test metrics: accuracy={'accuracy': 0.6825508058864751}
Epoch 3 training:
 ===> Epoch 3
 - Average training metrics: accuracy={'accuracy': 0.711}
 - Average validation metrics: accuracy={'accuracy': 0.6926605504587156}
 - Average test metrics: accuracy={'accuracy': 0.6993693062368606}
Epoch 4 training:
 ===> Epoch 4
 - Average training metrics: accuracy={'accuracy': 0.751875}
 - Average validation metrics: accuracy={'accuracy': 0.692354740061162}
 - Average test metrics: accuracy={'accuracy': 0.7119831814996496}
Epoch 5 training:
 ===> Epoch 5
 - Average training metrics: accuracy={'accuracy': 0.789875}
 - Average validation metrics: accuracy={'accuracy': 0.6902140672782875}
 - Average test metrics: accuracy={'accuracy': 0.7091800981079187}
Epoch 6 training:
 ===> Epoch 6
 - Average training metrics: accuracy={'accuracy': 0.813125}
 - Average validation metrics: accuracy={'accuracy': 0.6978593272171254}
 - Average test metrics: accuracy={'accuracy': 0.7070777855641205}
Epoch 7 training:
 ===> Epoch 7
 - Average training metrics: accuracy={'accuracy': 0.832375}
 - Average validation metrics: accuracy={'accuracy': 0.6990825688073394}
 - Average test metrics: accuracy={'accuracy': 0.708479327259986}
Epoch 8 training:
 ===> Epoch 8
 - Average training metrics: accuracy={'accuracy': 0.841875}
 - Average validation metrics: accuracy={'accuracy': 0.6987767584097859}
 - Average test metrics: accuracy={'accuracy': 0.7042747021723896}
Epoch 9 training:
 ===> Epoch 9
 - Average training metrics: accuracy={'accuracy': 0.8575}
 - Average validation metrics: accuracy={'accuracy': 0.7021406727828746}
 - Average test metrics: accuracy={'accuracy': 0.713384723195515}
torch.cuda.memory_allocated: 0.450818GB
torch.cuda.memory_reserved: 4.556641GB
torch.cuda.max_memory_reserved: 4.556641GB
Sun Mar 12 22:11:12 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.56       Driver Version: 460.56       CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Graphics Device     On   | 00000000:17:00.0 Off |                   On |
| N/A   59C    P0   278W / 300W |  54159MiB / 81251MiB |     N/A      Default |
|                               |                      |              Enabled |
+-------------------------------+----------------------+----------------------+
|   1  Graphics Device     On   | 00000000:31:00.0 Off |                   On |
| N/A   63C    P0   297W / 300W |  59239MiB / 81251MiB |     N/A      Default |
|                               |                      |              Enabled |
+-------------------------------+----------------------+----------------------+
|   2  Graphics Device     On   | 00000000:B1:00.0 Off |                   On |
| N/A   36C    P0    86W / 300W |   4769MiB / 81251MiB |     N/A      Default |
|                               |                      |              Enabled |
+-------------------------------+----------------------+----------------------+
|   3  Graphics Device     On   | 00000000:CA:00.0 Off |                   On |
| N/A   28C    P0    43W / 300W |     13MiB / 81251MiB |     N/A      Default |
|                               |                      |              Enabled |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| MIG devices:                                                                |
+------------------+----------------------+-----------+-----------------------+
| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |
|      ID  ID  Dev |           BAR1-Usage | SM     Unc| CE  ENC  DEC  OFA  JPG|
|                  |                      |        ECC|                       |
|==================+======================+===========+=======================|
|  0    7   0   0  |   5588MiB /  9728MiB | 14      0 |  1   0    0    0    0 |
|                  |      4MiB / 16383MiB |           |                       |
+------------------+----------------------+-----------+-----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0    7    0      32105      C   ...jeefzx3kp5jqdu/bin/python     5583MiB |
|    1   10    0      10051      C   ...sification_env/bin/python     9577MiB |
|    1    7    0      33369      C   ...jeefzx3kp5jqdu/bin/python     8271MiB |
|    1    8    0      33470      C   ...jeefzx3kp5jqdu/bin/python     8271MiB |
|    1    9    0      33627      C   ...jeefzx3kp5jqdu/bin/python     8271MiB |
|    1   11    0      33726      C   ...jeefzx3kp5jqdu/bin/python     8271MiB |
|    1   12    0      33827      C   ...jeefzx3kp5jqdu/bin/python     8271MiB |
|    1   13    0      33945      C   ...jeefzx3kp5jqdu/bin/python     8271MiB |
|    2    7    0      34557      C   ...jeefzx3kp5jqdu/bin/python     4753MiB |
+-----------------------------------------------------------------------------+

 - Average DEV metrics: accuracy={'accuracy': 0.7021406727828746}
 - Average TEST metrics: accuracy={'accuracy': 0.713384723195515}
Found cached dataset boolq (/home/tli104/.cache/huggingface/datasets/boolq/default/0.1.0/bf0dd57da941c50de94ae3ce3cef7fea48c08f337a4b7aac484e9dddc5aa24e5)
Specified arguments: Namespace(batch_size=32, device='cuda', experiment='full', lr=0.0005, model='t5-small', num_epochs=9, small_subset=False)
Loading the dataset ...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 866.05it/s]
/home/tli104/.local/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.
For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.
- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.
- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.
- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.
  warnings.warn(
Slicing the data...
Size of the loaded dataset:
 - train: 8000
 - dev: 3270
 - test: 1427
Loading the tokenizer...
Loding the data into DS...
 >>>>>>>> Initializing the data loaders ... 
Loading the model ...
Moving model to device ...cuda
 >>>>>>>>  Starting training ... 
 >>>>>>>>  Initializing optimizer
Epoch 1 training:
 ===> Epoch 1
 - Average training metrics: accuracy={'accuracy': 0.641625}
 - Average validation metrics: accuracy={'accuracy': 0.6636085626911316}
 - Average test metrics: accuracy={'accuracy': 0.6643307638402243}
Epoch 2 training:
 ===> Epoch 2
 - Average training metrics: accuracy={'accuracy': 0.704625}
 - Average validation metrics: accuracy={'accuracy': 0.6798165137614679}
 - Average test metrics: accuracy={'accuracy': 0.6762438682550805}
Epoch 3 training:
 ===> Epoch 3
 - Average training metrics: accuracy={'accuracy': 0.791125}
 - Average validation metrics: accuracy={'accuracy': 0.7003058103975535}
 - Average test metrics: accuracy={'accuracy': 0.6853538892782061}
Epoch 4 training:
 ===> Epoch 4
 - Average training metrics: accuracy={'accuracy': 0.873375}
 - Average validation metrics: accuracy={'accuracy': 0.6758409785932722}
 - Average test metrics: accuracy={'accuracy': 0.6825508058864751}
Epoch 5 training:
 ===> Epoch 5
 - Average training metrics: accuracy={'accuracy': 0.91825}
 - Average validation metrics: accuracy={'accuracy': 0.7021406727828746}
 - Average test metrics: accuracy={'accuracy': 0.693062368605466}
Epoch 6 training:
 ===> Epoch 6
 - Average training metrics: accuracy={'accuracy': 0.953875}
 - Average validation metrics: accuracy={'accuracy': 0.717737003058104}
 - Average test metrics: accuracy={'accuracy': 0.7028731604765242}
Epoch 7 training:
 ===> Epoch 7
 - Average training metrics: accuracy={'accuracy': 0.9675}
 - Average validation metrics: accuracy={'accuracy': 0.7122324159021407}
 - Average test metrics: accuracy={'accuracy': 0.6986685353889278}
Epoch 8 training:
 ===> Epoch 8
 - Average training metrics: accuracy={'accuracy': 0.98125}
 - Average validation metrics: accuracy={'accuracy': 0.7110091743119266}
 - Average test metrics: accuracy={'accuracy': 0.6993693062368606}
Epoch 9 training:
 ===> Epoch 9
 - Average training metrics: accuracy={'accuracy': 0.9845}
 - Average validation metrics: accuracy={'accuracy': 0.7140672782874617}
 - Average test metrics: accuracy={'accuracy': 0.7021723896285914}
torch.cuda.memory_allocated: 0.450818GB
torch.cuda.memory_reserved: 4.556641GB
torch.cuda.max_memory_reserved: 4.556641GB
Sun Mar 12 22:42:28 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.56       Driver Version: 460.56       CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Graphics Device     On   | 00000000:17:00.0 Off |                   On |
| N/A   59C    P0   260W / 300W |  41711MiB / 81251MiB |     N/A      Default |
|                               |                      |              Enabled |
+-------------------------------+----------------------+----------------------+
|   1  Graphics Device     On   | 00000000:31:00.0 Off |                   On |
| N/A   48C    P0   179W / 300W |  23054MiB / 81251MiB |     N/A      Default |
|                               |                      |              Enabled |
+-------------------------------+----------------------+----------------------+
|   2  Graphics Device     On   | 00000000:B1:00.0 Off |                   On |
| N/A   40C    P0   131W / 300W |   9622MiB / 81251MiB |     N/A      Default |
|                               |                      |              Enabled |
+-------------------------------+----------------------+----------------------+
|   3  Graphics Device     On   | 00000000:CA:00.0 Off |                   On |
| N/A   28C    P0    43W / 300W |     13MiB / 81251MiB |     N/A      Default |
|                               |                      |              Enabled |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| MIG devices:                                                                |
+------------------+----------------------+-----------+-----------------------+
| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |
|      ID  ID  Dev |           BAR1-Usage | SM     Unc| CE  ENC  DEC  OFA  JPG|
|                  |                      |        ECC|                       |
|==================+======================+===========+=======================|
|  0    7   0   0  |   5588MiB /  9728MiB | 14      0 |  1   0    0    0    0 |
|                  |      4MiB / 16383MiB |           |                       |
+------------------+----------------------+-----------+-----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0    7    0      35781      C   ...jeefzx3kp5jqdu/bin/python     5583MiB |
|    1   10    0      10051      C   ...sification_env/bin/python     9577MiB |
|    1   13    0      33945      C   ...jeefzx3kp5jqdu/bin/python     8271MiB |
|    1    7    0      39318      C   ...jeefzx3kp5jqdu/bin/python     5183MiB |
|    2    7    0      34557      C   ...jeefzx3kp5jqdu/bin/python     4753MiB |
|    2    8    0      36571      C   ...jeefzx3kp5jqdu/bin/python     4849MiB |
+-----------------------------------------------------------------------------+

 - Average DEV metrics: accuracy={'accuracy': 0.7140672782874617}
 - Average TEST metrics: accuracy={'accuracy': 0.7021723896285914}
Found cached dataset boolq (/home/tli104/.cache/huggingface/datasets/boolq/default/0.1.0/bf0dd57da941c50de94ae3ce3cef7fea48c08f337a4b7aac484e9dddc5aa24e5)
Specified arguments: Namespace(batch_size=32, device='cuda', experiment='full', lr=0.001, model='t5-small', num_epochs=9, small_subset=False)
Loading the dataset ...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 867.49it/s]
/home/tli104/.local/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.
For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.
- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.
- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.
- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.
  warnings.warn(
Slicing the data...
Size of the loaded dataset:
 - train: 8000
 - dev: 3270
 - test: 1427
Loading the tokenizer...
Loding the data into DS...
 >>>>>>>> Initializing the data loaders ... 
Loading the model ...
Moving model to device ...cuda
 >>>>>>>>  Starting training ... 
 >>>>>>>>  Initializing optimizer
Epoch 1 training:
 ===> Epoch 1
 - Average training metrics: accuracy={'accuracy': 0.614625}
 - Average validation metrics: accuracy={'accuracy': 0.6287461773700306}
 - Average test metrics: accuracy={'accuracy': 0.6454099509460406}
Epoch 2 training:
 ===> Epoch 2
 - Average training metrics: accuracy={'accuracy': 0.64975}
 - Average validation metrics: accuracy={'accuracy': 0.6452599388379205}
 - Average test metrics: accuracy={'accuracy': 0.6566222845129642}
Epoch 3 training:
 ===> Epoch 3
 - Average training metrics: accuracy={'accuracy': 0.6885}
 - Average validation metrics: accuracy={'accuracy': 0.6480122324159021}
 - Average test metrics: accuracy={'accuracy': 0.6580238262088297}
Epoch 4 training:
 ===> Epoch 4
 - Average training metrics: accuracy={'accuracy': 0.736625}
 - Average validation metrics: accuracy={'accuracy': 0.6602446483180429}
 - Average test metrics: accuracy={'accuracy': 0.6643307638402243}
Epoch 5 training:
 ===> Epoch 5
 - Average training metrics: accuracy={'accuracy': 0.789875}
 - Average validation metrics: accuracy={'accuracy': 0.6507645259938838}
 - Average test metrics: accuracy={'accuracy': 0.6615276804484933}
Epoch 6 training:
 ===> Epoch 6
 - Average training metrics: accuracy={'accuracy': 0.829375}
 - Average validation metrics: accuracy={'accuracy': 0.6565749235474007}
 - Average test metrics: accuracy={'accuracy': 0.6783461807988788}
Epoch 7 training:
 ===> Epoch 7
 - Average training metrics: accuracy={'accuracy': 0.8585}
 - Average validation metrics: accuracy={'accuracy': 0.6397553516819572}
 - Average test metrics: accuracy={'accuracy': 0.6601261387526279}
Epoch 8 training:
 ===> Epoch 8
 - Average training metrics: accuracy={'accuracy': 0.894125}
 - Average validation metrics: accuracy={'accuracy': 0.6311926605504588}
 - Average test metrics: accuracy={'accuracy': 0.6433076384022425}
Epoch 9 training:
 ===> Epoch 9
 - Average training metrics: accuracy={'accuracy': 0.9075}
 - Average validation metrics: accuracy={'accuracy': 0.6608562691131499}
 - Average test metrics: accuracy={'accuracy': 0.662228451296426}
torch.cuda.memory_allocated: 0.450818GB
torch.cuda.memory_reserved: 4.556641GB
torch.cuda.max_memory_reserved: 4.556641GB
Sun Mar 12 23:13:24 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.56       Driver Version: 460.56       CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Graphics Device     On   | 00000000:17:00.0 Off |                   On |
| N/A   60C    P0   260W / 300W |  41711MiB / 81251MiB |     N/A      Default |
|                               |                      |              Enabled |
+-------------------------------+----------------------+----------------------+
|   1  Graphics Device     On   | 00000000:31:00.0 Off |                   On |
| N/A   37C    P0   100W / 300W |   9593MiB / 81251MiB |     N/A      Default |
|                               |                      |              Enabled |
+-------------------------------+----------------------+----------------------+
|   2  Graphics Device     On   | 00000000:B1:00.0 Off |                   On |
| N/A   41C    P0   133W / 300W |   9622MiB / 81251MiB |     N/A      Default |
|                               |                      |              Enabled |
+-------------------------------+----------------------+----------------------+
|   3  Graphics Device     On   | 00000000:CA:00.0 Off |                   On |
| N/A   28C    P0    43W / 300W |     13MiB / 81251MiB |     N/A      Default |
|                               |                      |              Enabled |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| MIG devices:                                                                |
+------------------+----------------------+-----------+-----------------------+
| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |
|      ID  ID  Dev |           BAR1-Usage | SM     Unc| CE  ENC  DEC  OFA  JPG|
|                  |                      |        ECC|                       |
|==================+======================+===========+=======================|
|  0    7   0   0  |   5588MiB /  9728MiB | 14      0 |  1   0    0    0    0 |
|                  |      4MiB / 16383MiB |           |                       |
+------------------+----------------------+-----------+-----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0    7    0      39595      C   ...jeefzx3kp5jqdu/bin/python     5583MiB |
|    1   10    0      10051      C   ...sification_env/bin/python     9577MiB |
|    2    7    0      34557      C   ...jeefzx3kp5jqdu/bin/python     4753MiB |
|    2    8    0      36571      C   ...jeefzx3kp5jqdu/bin/python     4849MiB |
+-----------------------------------------------------------------------------+

 - Average DEV metrics: accuracy={'accuracy': 0.6608562691131499}
 - Average TEST metrics: accuracy={'accuracy': 0.662228451296426}
